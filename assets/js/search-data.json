{
  
    
        "post0": {
            "title": "Effective Tensorflow 2",
            "content": "View on TensorFlow.org | Run in Google Colab | View on GitHub | Download notebook | Overview . This guide provides a list of best practices for writing code using TensorFlow 2 (TF2), it is written for users who have recently switched over from TensorFlow 1 (TF1). Refer to the migrate section of the guide for more info on migrating your TF1 code to TF2. . Setup . Import TensorFlow and other dependencies for the examples in this guide. . import tensorflow as tf import tensorflow_datasets as tfds # pip install tensorflow_datasets print(&quot;tf.__version__&quot;) print(tf.__version__) . tf.__version__ 2.6.0 . Recommendations for idiomatic TensorFlow 2 . Refactor your code into smaller modules . A good practice is to refactor your code into smaller functions that are called as needed. For best performance, you should try to decorate the largest blocks of computation that you can in a tf.function (note that the nested python functions called by a tf.function do not require their own separate decorations, unless you want to use different jit_compile settings for the tf.function). Depending on your use case, this could be multiple training steps or even your whole training loop. For inference use cases, it might be a single model forward pass. . Adjust the default learning rate for some tf.keras.optimizers . Some Keras optimizers have different learning rates in TF2. If you see a change in convergence behavior for your models, check the default learning rates. . There are no changes for optimizers.SGD, optimizers.Adam, or optimizers.RMSprop. . The following default learning rates have changed: . optimizers.Adagrad from 0.01 to 0.001 | optimizers.Adadelta from 1.0 to 0.001 | optimizers.Adamax from 0.002 to 0.001 | optimizers.Nadam from 0.002 to 0.001 | . Use tf.Modules and Keras layers to manage variables . tf.Modules and tf.keras.layers.Layers offer the convenient variables and trainable_variables properties, which recursively gather up all dependent variables. This makes it easy to manage variables locally to where they are being used. . Keras layers/models inherit from tf.train.Checkpointable and are integrated with @tf.function, which makes it possible to directly checkpoint or export SavedModels from Keras objects. You do not necessarily have to use Keras&#39; Model.fit API to take advantage of these integrations. . Read the section on transfer learning and fine-tuning in the Keras guide to learn how to collect a subset of relevant variables using Keras. . Combine tf.data.Datasets and tf.function . The TensorFlow Datasets package (tfds) contains utilities for loading predefined datasets as tf.data.Dataset objects. For this example, you can load the MNIST dataset using tfds: . datasets, info = tfds.load(name=&#39;mnist&#39;, with_info=True, as_supervised=True) mnist_train, mnist_test = datasets[&#39;train&#39;], datasets[&#39;test&#39;] . 2022-09-05 22:09:54.002690: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with &#34;Not found: Could not locate the credentials file.&#34;. Retrieving token from GCE failed with &#34;Failed precondition: Error executing an HTTP request: libcurl code 6 meaning &#39;Couldn&#39;t resolve host name&#39;, error details: Could not resolve host: metadata&#34;. . Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to ~/tensorflow_datasets/mnist/3.0.1... Dataset mnist downloaded and prepared to ~/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data. . 2022-09-05 22:10:14.104904: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. . Then prepare the data for training: . Re-scale each image. | Shuffle the order of the examples. | Collect batches of images and labels. | . BUFFER_SIZE = 10 # Use a much larger value for real code BATCH_SIZE = 64 NUM_EPOCHS = 5 def scale(image, label): image = tf.cast(image, tf.float32) image /= 255 return image, label . To keep the example short, trim the dataset to only return 5 batches: . train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) test_data = mnist_test.map(scale).batch(BATCH_SIZE) STEPS_PER_EPOCH = 5 train_data = train_data.take(STEPS_PER_EPOCH) test_data = test_data.take(STEPS_PER_EPOCH) . image_batch, label_batch = next(iter(train_data)) . 2022-09-05 22:10:19.047400: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2) 2022-09-05 22:10:20.129283: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Use regular Python iteration to iterate over training data that fits in memory. Otherwise, tf.data.Dataset is the best way to stream training data from disk. Datasets are iterables (not iterators), and work just like other Python iterables in eager execution. You can fully utilize dataset async prefetching/streaming features by wrapping your code in tf.function, which replaces Python iteration with the equivalent graph operations using AutoGraph. . @tf.function def train(model, dataset, optimizer): for x, y in dataset: with tf.GradientTape() as tape: # training=True is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). prediction = model(x, training=True) loss = loss_fn(prediction, y) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) . If you use the Keras Model.fit API, you won&#39;t have to worry about dataset iteration. . model.compile(optimizer=optimizer, loss=loss_fn) model.fit(dataset) . . Use Keras training loops . If you don&#39;t need low-level control of your training process, using Keras&#39; built-in fit, evaluate, and predict methods is recommended. These methods provide a uniform interface to train the model regardless of the implementation (sequential, functional, or sub-classed). . The advantages of these methods include: . They accept Numpy arrays, Python generators and, tf.data.Datasets. | They apply regularization, and activation losses automatically. | They support tf.distribute where the training code remains the same regardless of the hardware configuration. | They support arbitrary callables as losses and metrics. | They support callbacks like tf.keras.callbacks.TensorBoard, and custom callbacks. | They are performant, automatically using TensorFlow graphs. | . Here is an example of training a model using a Dataset. For details on how this works, check out the tutorials. . model = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=&#39;relu&#39;, kernel_regularizer=tf.keras.regularizers.l2(0.02), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=&#39;relu&#39;), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ]) # Model is the full model w/o custom layers model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) model.fit(train_data, epochs=NUM_EPOCHS) loss, acc = model.evaluate(test_data) print(&quot;Loss {}, Accuracy {}&quot;.format(loss, acc)) . Epoch 1/5 5/5 [==============================] - 3s 31ms/step - loss: 1.5200 - accuracy: 0.5312 Epoch 2/5 1/5 [=====&gt;........................] - ETA: 0s - loss: 0.5561 - accuracy: 0.8594 . 2022-09-05 22:10:44.195383: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . 5/5 [==============================] - 0s 82ms/step - loss: 0.4627 - accuracy: 0.9031 Epoch 3/5 1/5 [=====&gt;........................] - ETA: 0s - loss: 0.3418 - accuracy: 0.9688 . 2022-09-05 22:10:44.618515: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . 5/5 [==============================] - 0s 82ms/step - loss: 0.2962 - accuracy: 0.9594 Epoch 4/5 . 2022-09-05 22:10:45.054931: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . 5/5 [==============================] - 0s 47ms/step - loss: 0.2400 - accuracy: 0.9656 Epoch 5/5 5/5 [==============================] - 0s 32ms/step - loss: 0.1822 - accuracy: 0.9812 . 2022-09-05 22:10:45.620667: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. 2022-09-05 22:10:45.811631: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . 5/5 [==============================] - 1s 23ms/step - loss: 1.5268 - accuracy: 0.7312 Loss 1.5267544984817505, Accuracy 0.731249988079071 . 2022-09-05 22:10:47.116555: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . . Customize training and write your own loop . If Keras models work for you, but you need more flexibility and control of the training step or the outer training loops, you can implement your own training steps or even entire training loops. See the Keras guide on customizing fit to learn more. . You can also implement many things as a tf.keras.callbacks.Callback. . This method has many of the advantages mentioned previously, but gives you control of the train step and even the outer loop. . There are three steps to a standard training loop: . Iterate over a Python generator or tf.data.Dataset to get batches of examples. | Use tf.GradientTape to collect gradients. | Use one of the tf.keras.optimizers to apply weight updates to the model&#39;s variables. | Remember: . Always include a training argument on the call method of subclassed layers and models. | Make sure to call the model with the training argument set correctly. | Depending on usage, model variables may not exist until the model is run on a batch of data. | You need to manually handle things like regularization losses for the model. | . There is no need to run variable initializers or to add manual control dependencies. tf.function handles automatic control dependencies and variable initialization on creation for you. . model = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=&#39;relu&#39;, kernel_regularizer=tf.keras.regularizers.l2(0.02), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=&#39;relu&#39;), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ]) optimizer = tf.keras.optimizers.Adam(0.001) loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) @tf.function def train_step(inputs, labels): with tf.GradientTape() as tape: predictions = model(inputs, training=True) regularization_loss=tf.math.add_n(model.losses) pred_loss=loss_fn(labels, predictions) total_loss=pred_loss + regularization_loss gradients = tape.gradient(total_loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) for epoch in range(NUM_EPOCHS): for inputs, labels in train_data: train_step(inputs, labels) print(&quot;Finished epoch&quot;, epoch) . 2022-09-05 22:10:51.610468: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. 2022-09-05 22:10:51.784033: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Finished epoch 0 Finished epoch 1 . 2022-09-05 22:10:52.092303: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Finished epoch 2 . 2022-09-05 22:10:52.772250: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Finished epoch 3 Finished epoch 4 . 2022-09-05 22:10:53.439140: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Take advantage of tf.function with Python control flow . tf.function provides a way to convert data-dependent control flow into graph-mode equivalents like tf.cond and tf.while_loop. . One common place where data-dependent control flow appears is in sequence models. tf.keras.layers.RNN wraps an RNN cell, allowing you to either statically or dynamically unroll the recurrence. As an example, you could reimplement dynamic unroll as follows. . class DynamicRNN(tf.keras.Model): def __init__(self, rnn_cell): super(DynamicRNN, self).__init__(self) self.cell = rnn_cell @tf.function(input_signature=[tf.TensorSpec(dtype=tf.float32, shape=[None, None, 3])]) def call(self, input_data): # [batch, time, features] -&gt; [time, batch, features] input_data = tf.transpose(input_data, [1, 0, 2]) timesteps = tf.shape(input_data)[0] batch_size = tf.shape(input_data)[1] outputs = tf.TensorArray(tf.float32, timesteps) state = self.cell.get_initial_state(batch_size = batch_size, dtype=tf.float32) for i in tf.range(timesteps): output, state = self.cell(input_data[i], state) outputs = outputs.write(i, output) return tf.transpose(outputs.stack(), [1, 0, 2]), state . lstm_cell = tf.keras.layers.LSTMCell(units = 13) my_rnn = DynamicRNN(lstm_cell) outputs, state = my_rnn(tf.random.normal(shape=[10,20,3])) print(outputs.shape) . (10, 20, 13) . Read the tf.function guide for a more information. . New-style metrics and losses . Metrics and losses are both objects that work eagerly and in tf.functions. . A loss object is callable, and expects (y_true, y_pred) as arguments: . cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True) cce([[1, 0]], [[-1.0,3.0]]).numpy() . 4.01815 . Use metrics to collect and display data . You can use tf.metrics to aggregate data and tf.summary to log summaries and redirect it to a writer using a context manager. The summaries are emitted directly to the writer which means that you must provide the step value at the callsite. . summary_writer = tf.summary.create_file_writer(&#39;/tmp/summaries&#39;) with summary_writer.as_default(): tf.summary.scalar(&#39;loss&#39;, 0.1, step=42) . Use tf.metrics to aggregate data before logging them as summaries. Metrics are stateful; they accumulate values and return a cumulative result when you call the result method (such as Mean.result). Clear accumulated values with Model.reset_states. . def train(model, optimizer, dataset, log_freq=10): avg_loss = tf.keras.metrics.Mean(name=&#39;loss&#39;, dtype=tf.float32) for images, labels in dataset: loss = train_step(model, optimizer, images, labels) avg_loss.update_state(loss) if tf.equal(optimizer.iterations % log_freq, 0): tf.summary.scalar(&#39;loss&#39;, avg_loss.result(), step=optimizer.iterations) avg_loss.reset_states() def test(model, test_x, test_y, step_num): # training=False is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). loss = loss_fn(model(test_x, training=False), test_y) tf.summary.scalar(&#39;loss&#39;, loss, step=step_num) train_summary_writer = tf.summary.create_file_writer(&#39;/tmp/summaries/train&#39;) test_summary_writer = tf.summary.create_file_writer(&#39;/tmp/summaries/test&#39;) with train_summary_writer.as_default(): train(model, optimizer, dataset) with test_summary_writer.as_default(): test(model, test_x, test_y, optimizer.iterations) . Visualize the generated summaries by pointing TensorBoard to the summary log directory: . tensorboard --logdir /tmp/summaries . Use the tf.summary API to write summary data for visualization in TensorBoard. For more info, read the tf.summary guide. . loss_metric = tf.keras.metrics.Mean(name=&#39;train_loss&#39;) accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name=&#39;train_accuracy&#39;) @tf.function def train_step(inputs, labels): with tf.GradientTape() as tape: predictions = model(inputs, training=True) regularization_loss=tf.math.add_n(model.losses) pred_loss=loss_fn(labels, predictions) total_loss=pred_loss + regularization_loss gradients = tape.gradient(total_loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # Update the metrics loss_metric.update_state(total_loss) accuracy_metric.update_state(labels, predictions) for epoch in range(NUM_EPOCHS): # Reset the metrics loss_metric.reset_states() accuracy_metric.reset_states() for inputs, labels in train_data: train_step(inputs, labels) # Get the metric results mean_loss=loss_metric.result() mean_accuracy = accuracy_metric.result() print(&#39;Epoch: &#39;, epoch) print(&#39; loss: {:.3f}&#39;.format(mean_loss)) print(&#39; accuracy: {:.3f}&#39;.format(mean_accuracy)) . 2022-09-05 22:11:02.258069: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Epoch: 0 loss: 0.135 accuracy: 0.994 . 2022-09-05 22:11:02.514210: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. 2022-09-05 22:11:02.702151: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Epoch: 1 loss: 0.117 accuracy: 0.997 Epoch: 2 loss: 0.098 accuracy: 1.000 . 2022-09-05 22:11:03.212173: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Epoch: 3 loss: 0.081 accuracy: 1.000 Epoch: 4 loss: 0.076 accuracy: 1.000 . 2022-09-05 22:11:03.820060: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . Keras metric names . Keras models are consistent about handling metric names. When you pass a string in the list of metrics, that exact string is used as the metric&#39;s name. These names are visible in the history object returned by model.fit, and in the logs passed to keras.callbacks. is set to the string you passed in the metric list. . model.compile( optimizer = tf.keras.optimizers.Adam(0.001), loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = [&#39;acc&#39;, &#39;accuracy&#39;, tf.keras.metrics.SparseCategoricalAccuracy(name=&quot;my_accuracy&quot;)]) history = model.fit(train_data) . 5/5 [==============================] - 2s 21ms/step - loss: 0.0800 - acc: 1.0000 - accuracy: 1.0000 - my_accuracy: 1.0000 . 2022-09-05 22:11:06.662527: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. . history.history.keys() . dict_keys([&#39;loss&#39;, &#39;acc&#39;, &#39;accuracy&#39;, &#39;my_accuracy&#39;]) . Debugging . Use eager execution to run your code step-by-step to inspect shapes, data types and values. Certain APIs, like tf.function, tf.keras, etc. are designed to use Graph execution, for performance and portability. When debugging, use tf.config.run_functions_eagerly(True) to use eager execution inside this code. . For example: . @tf.function def f(x): if x &gt; 0: import pdb pdb.set_trace() x = x + 1 return x tf.config.run_functions_eagerly(True) f(tf.constant(1)) . &gt; &gt;&gt; f() -&gt; x = x + 1 (Pdb) l 6 @tf.function 7 def f(x):8 if x &gt; 0: 9 import pdb 10 pdb.set_trace() 11 -&gt; x = x + 1 12 return x 13 14 tf.config.run_functions_eagerly(True) 15 f(tf.constant(1)) [EOF] . This also works inside Keras models and other APIs that support eager execution: . class CustomModel(tf.keras.models.Model): @tf.function def call(self, input_data): if tf.reduce_mean(input_data) &gt; 0: return input_data else: import pdb pdb.set_trace() return input_data // 2 tf.config.run_functions_eagerly(True) model = CustomModel() model(tf.constant([-2, -4])) . &gt; &gt;&gt; call() -&gt; return input_data // 2 (Pdb) l 10 if tf.reduce_mean(input_data) &gt; 0:11 return input_data 12 else: 13 import pdb 14 pdb.set_trace() 15 -&gt; return input_data // 2 16 17 18 tf.config.run_functions_eagerly(True) 19 model = CustomModel() 20 model(tf.constant([-2, -4])) . Notes: . tf.keras.Model methods such as fit, evaluate, and predict execute as graphs with tf.function under the hood. . | When using tf.keras.Model.compile, set run_eagerly = True to disable the Model logic from being wrapped in a tf.function. . | Use tf.data.experimental.enable_debug_mode to enable the debug mode for tf.data. Read the API docs for more details. . | . Do not keep tf.Tensors in your objects . These tensor objects might get created either in a tf.function or in the eager context, and these tensors behave differently. Always use tf.Tensors only for intermediate values. . To track state, use tf.Variables as they are always usable from both contexts. Read the tf.Variable guide to learn more. . Resources and further reading . Read the TF2 guides and tutorials to learn more about how to use TF2. . | If you previously used TF1.x, it is highly recommended you migrate your code to TF2. Read the migration guides to learn more. . | .",
            "url": "https://taocao.github.io/ml/jupyter/2022/09/05/effective_tf2.html",
            "relUrl": "/jupyter/2022/09/05/effective_tf2.html",
            "date": " • Sep 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Text Classification With Python and Keras",
            "content": "Importing Packages . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import RandomizedSearchCV import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras import layers from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.wrappers.scikit_learn import KerasClassifier import os plt.style.use(&#39;ggplot&#39;) . Extract the folder into a data folder and go ahead and load the data with Pandas: . filepath_dict = {&#39;yelp&#39;: &#39;../data/sentiment_labelled_sentences/yelp_labelled.txt&#39;, &#39;amazon&#39;: &#39;../data/sentiment_labelled_sentences/amazon_cells_labelled.txt&#39;, &#39;imdb&#39;: &#39;../data/sentiment_labelled_sentences/imdb_labelled.txt&#39;} df_list = [] for source, filepath in filepath_dict.items(): df = pd.read_csv(filepath, names=[&#39;sentence&#39;, &#39;label&#39;], sep=&#39; t&#39;) df[&#39;source&#39;] = source df_list.append(df) . len(df_list) . 3 . df_list . [ sentence label source 0 Wow... Loved this place. 1 yelp 1 Crust is not good. 0 yelp 2 Not tasty and the texture was just nasty. 0 yelp 3 Stopped by during the late May bank holiday of... 1 yelp 4 The selection on the menu was great and so wer... 1 yelp .. ... ... ... 995 I think food should have flavor and texture an... 0 yelp 996 Appetite instantly gone. 0 yelp 997 Overall I was not impressed and would not go b... 0 yelp 998 The whole experience was underwhelming, and I ... 0 yelp 999 Then, as if I hadn&#39;t wasted enough of my life ... 0 yelp [1000 rows x 3 columns], sentence label source 0 So there is no way for me to plug it in here i... 0 amazon 1 Good case, Excellent value. 1 amazon 2 Great for the jawbone. 1 amazon 3 Tied to charger for conversations lasting more... 0 amazon 4 The mic is great. 1 amazon .. ... ... ... 995 The screen does get smudged easily because it ... 0 amazon 996 What a piece of junk.. I lose more calls on th... 0 amazon 997 Item Does Not Match Picture. 0 amazon 998 The only thing that disappoint me is the infra... 0 amazon 999 You can not answer calls with the unit, never ... 0 amazon [1000 rows x 3 columns], sentence label source 0 A very, very, very slow-moving, aimless movie ... 0 imdb 1 Not sure who was more lost - the flat characte... 0 imdb 2 Attempting artiness with black &amp; white and cle... 0 imdb 3 Very little music or anything to speak of. 0 imdb 4 The best scene in the movie was when Gerardo i... 1 imdb .. ... ... ... 743 I just got bored watching Jessice Lange take h... 0 imdb 744 Unfortunately, any virtue in this film&#39;s produ... 0 imdb 745 In a word, it is embarrassing. 0 imdb 746 Exceptionally bad! 0 imdb 747 All in all its an insult to one&#39;s intelligence... 0 imdb [748 rows x 3 columns]] . df = pd.concat(df_list) df.iloc[0] . sentence Wow... Loved this place. label 1 source yelp Name: 0, dtype: object . df.head() . sentence label source . 0 Wow... Loved this place. | 1 | yelp | . 1 Crust is not good. | 0 | yelp | . 2 Not tasty and the texture was just nasty. | 0 | yelp | . 3 Stopped by during the late May bank holiday of... | 1 | yelp | . 4 The selection on the menu was great and so wer... | 1 | yelp | . df.tail() . sentence label source . 743 I just got bored watching Jessice Lange take h... | 0 | imdb | . 744 Unfortunately, any virtue in this film&#39;s produ... | 0 | imdb | . 745 In a word, it is embarrassing. | 0 | imdb | . 746 Exceptionally bad! | 0 | imdb | . 747 All in all its an insult to one&#39;s intelligence... | 0 | imdb | . Now use the CountVectorizer provided by the scikit-learn library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words: . sentences = [&#39;Rashmi likes ice cream&#39;, &#39;Rashmi hates chocolate.&#39;] . vectorizer = CountVectorizer(min_df=0, lowercase=False) vectorizer.fit(sentences) vectorizer.vocabulary_ . {&#39;Rashmi&#39;: 0, &#39;likes&#39;: 5, &#39;ice&#39;: 4, &#39;cream&#39;: 2, &#39;hates&#39;: 3, &#39;chocolate&#39;: 1} . vectorizer.vocabulary_.get(u&#39;ice&#39;) . 4 . d = vectorizer.vocabulary_ {key:d[key] for key in sorted(d.keys())} . {&#39;Rashmi&#39;: 0, &#39;chocolate&#39;: 1, &#39;cream&#39;: 2, &#39;hates&#39;: 3, &#39;ice&#39;: 4, &#39;likes&#39;: 5} . vocabulary_list=[[key for key in sorted(d.keys())],[d[key] for key in sorted(d.keys())]] . vocabulary_list . [[&#39;Rashmi&#39;, &#39;chocolate&#39;, &#39;cream&#39;, &#39;hates&#39;, &#39;ice&#39;, &#39;likes&#39;], [0, 1, 2, 3, 4, 5]] . vectorizer.transform(sentences).toarray() . array([[1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 0, 0]]) . vectorizer.transform(sentences).toarray()[1] . array([1, 1, 0, 1, 0, 0]) . vectors = vectorizer.transform(sentences).toarray().tolist() vectors . [[1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 0, 0]] . data = [vocabulary_list[0],vectors[0],vectors[1]] pd.DataFrame(data) . 0 1 2 3 4 5 . 0 Rashmi | chocolate | cream | hates | ice | likes | . 1 1 | 0 | 1 | 0 | 1 | 1 | . 2 1 | 1 | 0 | 1 | 0 | 0 | . Extracting features from text files . In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors. . Bags of words The most intuitive way to do so is to use a bags of words representation: . Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices). . Tokenizing text with scikit-learn&#182; . Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors . The bags of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000. . If n_samples == 10000, storing X as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = 4GB in RAM which is barely manageable on today&#8217;s computers. . Fortunately, most values in X will be zeros since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory. . scipy.sparse matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures. . From occurrences to frequencies&#182; . Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics. . To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies. . Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus. . This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”. . Both tf and tf–idf can be computed as follows using TfidfTransformer: . Defining a Baseline Model . First, you are going to split the data into a training and testing set which will allow you to evaluate the accuracy and see if your model generalizes well. This means whether the model is able to perform well on data it has not seen before. This is a way to see if the model is overfitting. . Overfitting is when a model is trained too well on the training data. You want to avoid overfitting, as this would mean that the model mostly just memorized the training data. This would account for a large accuracy with the training data but a low accuracy in the testing data. . We start by taking the Yelp data set which we extract from our concatenated data set. From there, we take the sentences and labels. . df_yelp = df[df[&#39;source&#39;] == &#39;yelp&#39;] sentences = df_yelp[&#39;sentence&#39;].values y = df_yelp[&#39;label&#39;].values sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000) . Create the feature vectors for each sentence of the training and testing set: . vectorizer = CountVectorizer() vectorizer.fit(sentences_train) X_train = vectorizer.transform(sentences_train) X_test = vectorizer.transform(sentences_test) . X_train . &lt;750x1714 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39; with 7368 stored elements in Compressed Sparse Row format&gt; . CountVectorizer performs tokenization which separates the sentences into a set of tokens. It additionally removes punctuation and special characters and can apply other preprocessing to each word. If you want, you can use a custom tokenizer from the NLTK library with the CountVectorizer or use any number of the customizations which you can explore to improve the performance of your model. . The classification model we are going to use is the logistic regression which is a simple yet powerful linear model that is mathematically speaking in fact a form of regression between 0 and 1 based on the input feature vector. By specifying a cutoff value (by default 0.5), the regression model is used for classification. . classifier = LogisticRegression() classifier.fit(X_train, y_train) score = classifier.score(X_test, y_test) print(&quot;Accuracy:&quot;, score) . Accuracy: 0.796 . You can see that the logistic regression reached an impressive 79.6%, but let’s have a look how this model performs on the other data sets that we have. In this script, we perform and evaluate the whole process for each data set that we have: . for source in df[&#39;source&#39;].unique(): df_source = df[df[&#39;source&#39;] == source] sentences = df_source[&#39;sentence&#39;].values y = df_source[&#39;label&#39;].values sentences_train, sentences_test, y_train, y_test = train_test_split( sentences, y, test_size=0.25, random_state=1000) vectorizer = CountVectorizer() vectorizer.fit(sentences_train) X_train = vectorizer.transform(sentences_train) X_test = vectorizer.transform(sentences_test) classifier = LogisticRegression() classifier.fit(X_train, y_train) score = classifier.score(X_test, y_test) print(&#39;Accuracy for {} data: {:.4f}&#39;.format(source, score)) . Accuracy for yelp data: 0.7960 Accuracy for amazon data: 0.7960 Accuracy for imdb data: 0.7487 . Great! You can see that this fairly simple model achieves a fairly good accuracy. . from sklearn.feature_extraction.text import TfidfTransformer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline text_clf = Pipeline([ (&#39;vect&#39;, CountVectorizer()), (&#39;clf&#39;, LogisticRegression()), ]) for source in df[&#39;source&#39;].unique(): df_source = df[df[&#39;source&#39;] == source] sentences = df_source[&#39;sentence&#39;].values y = df_source[&#39;label&#39;].values sentences_train, sentences_test, y_train, y_test = train_test_split( sentences, y, test_size=0.25, random_state=1000) text_clf.fit(sentences_train, y_train) score = text_clf.score(sentences_test, y_test) print(&#39;Model {} Accuracy for {} data: {:.4f}&#39;.format(LogisticRegression,source, score)) . Model &lt;class &#39;sklearn.linear_model._logistic.LogisticRegression&#39;&gt; Accuracy for yelp data: 0.7960 Model &lt;class &#39;sklearn.linear_model._logistic.LogisticRegression&#39;&gt; Accuracy for amazon data: 0.7960 Model &lt;class &#39;sklearn.linear_model._logistic.LogisticRegression&#39;&gt; Accuracy for imdb data: 0.7487 . try a new feature engineering method . from sklearn.feature_extraction.text import TfidfTransformer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline text_clf = Pipeline([ (&#39;vect&#39;, CountVectorizer()), (&#39;tfidf&#39;, TfidfTransformer()), (&#39;clf&#39;, LogisticRegression()), ]) for source in df[&#39;source&#39;].unique(): df_source = df[df[&#39;source&#39;] == source] sentences = df_source[&#39;sentence&#39;].values y = df_source[&#39;label&#39;].values sentences_train, sentences_test, y_train, y_test = train_test_split( sentences, y, test_size=0.25, random_state=1000) text_clf.fit(sentences_train, y_train) score = text_clf.score(sentences_test, y_test) print(&#39;Model {} Accuracy for {} data: {:.4f}&#39;.format(LogisticRegression,source, score)) . Model &lt;class &#39;sklearn.linear_model._logistic.LogisticRegression&#39;&gt; Accuracy for yelp data: 0.7680 Model &lt;class &#39;sklearn.linear_model._logistic.LogisticRegression&#39;&gt; Accuracy for amazon data: 0.8000 Model &lt;class &#39;sklearn.linear_model._logistic.LogisticRegression&#39;&gt; Accuracy for imdb data: 0.7380 . from sklearn.feature_extraction.text import TfidfTransformer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline text_clf = Pipeline([ (&#39;vect&#39;, CountVectorizer()), (&#39;tfidf&#39;, TfidfTransformer()), (&#39;clf&#39;, MultinomialNB()), ]) for source in df[&#39;source&#39;].unique(): df_source = df[df[&#39;source&#39;] == source] sentences = df_source[&#39;sentence&#39;].values y = df_source[&#39;label&#39;].values sentences_train, sentences_test, y_train, y_test = train_test_split( sentences, y, test_size=0.25, random_state=1000) text_clf.fit(sentences_train, y_train) score = text_clf.score(sentences_test, y_test) print(&#39;Model {} Accuracy for {} data: {:.4f}&#39;.format(MultinomialNB,source, score)) . Model &lt;class &#39;sklearn.naive_bayes.MultinomialNB&#39;&gt; Accuracy for yelp data: 0.7680 Model &lt;class &#39;sklearn.naive_bayes.MultinomialNB&#39;&gt; Accuracy for amazon data: 0.8000 Model &lt;class &#39;sklearn.naive_bayes.MultinomialNB&#39;&gt; Accuracy for imdb data: 0.7914 . Introduction to Deep Neural Networks . Neural networks, or sometimes called artificial neural network (ANN) orfeedforward neural network, are computational networks which were vaguely inspired by the neural networks in the human brain. They consist of neurons (also called nodes) which are connected like in the graph below. . You start by having a layer of input neurons where you feed in your feature vectors and the values are then feeded forward to a hidden layer. At each connection, you are feeding the value forward, while the value is multiplied by a weight and a bias is added to the value. This happens at every connection and at the end you reach an output layer with one or more output nodes. . If you want to have a binary classification you can use one node, but if you have multiple categories you should use multiple nodes for each category: . Introducing Keras . Keras is a deep learning and neural networks API by François Chollet which is capable of running on top of Tensorflow (Google), Theano or CNTK (Microsoft). To quote the wonderful book by François Chollet, Deep Learning with Python: . Keras is a model-level library, providing high-level building blocks for developing deep-learning models. It doesn’t handle low-level operations such as tensor manipulation and differentiation. Instead, it relies on a specialized, well-optimized tensor library to do so, serving as the backend engine of Keras (Source) . It is a great way to start experimenting with neural networks without having to implement every layer and piece on your own. For example Tensorflow is a great machine learning library, but you have to implement a lot of boilerplate code to have a model running. . First Keras Model . Keras supports two main types of models. You have the Sequential model API and the functional API which can do everything of the Sequential model but it can be also used for advanced models with complex network architectures. . The Sequential model is a linear stack of layers, where you can use the large variety of available layers in Keras. The most common layer is the Dense layer which is your regular densely connected neural network layer with all the weights and biases that you are already familiar with. . Before we build our model, we need to know the input dimension of our feature vectors. This happens only in the first layer since the following layers can do automatic shape inference. In order to build the Sequential model, you can add layers one by one in order . input_dim = X_train.shape[1] # Number of features model = Sequential() model.add(layers.Dense(10, input_dim=input_dim, activation=&#39;relu&#39;)) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) . 2022-06-01 23:34:10.474167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. . model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 10) 25060 dense_1 (Dense) (None, 1) 11 ================================================================= Total params: 25,071 Trainable params: 25,071 Non-trainable params: 0 _________________________________________________________________ . history = model.fit(X_train, y_train, epochs=100, verbose=True, validation_data=(X_test, y_test), batch_size=10) . Epoch 1/100 . /Users/tc390714/.mle_app_demo/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(&#34;gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0&#34;, shape=(None,), dtype=int32), values=Tensor(&#34;gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0&#34;, shape=(None, 10), dtype=float32), dense_shape=Tensor(&#34;gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0&#34;, shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory. warnings.warn( . 57/57 [==============================] - 1s 8ms/step - loss: 0.6894 - accuracy: 0.5544 - val_loss: 0.6889 - val_accuracy: 0.5401 Epoch 2/100 57/57 [==============================] - 0s 2ms/step - loss: 0.6278 - accuracy: 0.7807 - val_loss: 0.6508 - val_accuracy: 0.6524 Epoch 3/100 57/57 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.8752 - val_loss: 0.6624 - val_accuracy: 0.7112 Epoch 4/100 57/57 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.9305 - val_loss: 0.5891 - val_accuracy: 0.7540 Epoch 5/100 57/57 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.9590 - val_loss: 0.5773 - val_accuracy: 0.7647 Epoch 6/100 57/57 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.9750 - val_loss: 0.5517 - val_accuracy: 0.7647 Epoch 7/100 57/57 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.9875 - val_loss: 0.5524 - val_accuracy: 0.7754 Epoch 8/100 57/57 [==============================] - 0s 4ms/step - loss: 0.1943 - accuracy: 0.9911 - val_loss: 0.5472 - val_accuracy: 0.7754 Epoch 9/100 57/57 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.9911 - val_loss: 0.5165 - val_accuracy: 0.7914 Epoch 10/100 57/57 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9929 - val_loss: 0.5256 - val_accuracy: 0.7861 Epoch 11/100 57/57 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9982 - val_loss: 0.5253 - val_accuracy: 0.7861 Epoch 12/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0987 - accuracy: 0.9982 - val_loss: 0.5190 - val_accuracy: 0.7914 Epoch 13/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9982 - val_loss: 0.5220 - val_accuracy: 0.7968 Epoch 14/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9982 - val_loss: 0.5348 - val_accuracy: 0.8021 Epoch 15/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9982 - val_loss: 0.5236 - val_accuracy: 0.8021 Epoch 16/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0579 - accuracy: 0.9982 - val_loss: 0.5420 - val_accuracy: 0.7914 Epoch 17/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 0.9982 - val_loss: 0.5293 - val_accuracy: 0.7968 Epoch 18/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9982 - val_loss: 0.5447 - val_accuracy: 0.7968 Epoch 19/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9982 - val_loss: 0.5415 - val_accuracy: 0.8021 Epoch 20/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9982 - val_loss: 0.5560 - val_accuracy: 0.7968 Epoch 21/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9982 - val_loss: 0.5582 - val_accuracy: 0.7914 Epoch 22/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0308 - accuracy: 0.9982 - val_loss: 0.5587 - val_accuracy: 0.7968 Epoch 23/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.5592 - val_accuracy: 0.8021 Epoch 24/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 0.9982 - val_loss: 0.5739 - val_accuracy: 0.7968 Epoch 25/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9982 - val_loss: 0.5764 - val_accuracy: 0.7968 Epoch 26/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0216 - accuracy: 0.9982 - val_loss: 0.5821 - val_accuracy: 0.7968 Epoch 27/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.9982 - val_loss: 0.5914 - val_accuracy: 0.7968 Epoch 28/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 0.9982 - val_loss: 0.5985 - val_accuracy: 0.7968 Epoch 29/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9982 - val_loss: 0.6061 - val_accuracy: 0.7914 Epoch 30/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0159 - accuracy: 0.9982 - val_loss: 0.6023 - val_accuracy: 0.7914 Epoch 31/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9982 - val_loss: 0.6047 - val_accuracy: 0.7914 Epoch 32/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9982 - val_loss: 0.6179 - val_accuracy: 0.7861 Epoch 33/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 0.7861 Epoch 34/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 0.7861 Epoch 35/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.6351 - val_accuracy: 0.7861 Epoch 36/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.6384 - val_accuracy: 0.7861 Epoch 37/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.6393 - val_accuracy: 0.7861 Epoch 38/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.6434 - val_accuracy: 0.7861 Epoch 39/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.6580 - val_accuracy: 0.7861 Epoch 40/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.6609 - val_accuracy: 0.7861 Epoch 41/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6689 - val_accuracy: 0.7861 Epoch 42/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 0.7861 Epoch 43/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.7914 Epoch 44/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.6665 - val_accuracy: 0.7914 Epoch 45/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6821 - val_accuracy: 0.7914 Epoch 46/100 57/57 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.6909 - val_accuracy: 0.7914 Epoch 47/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.6952 - val_accuracy: 0.7914 Epoch 48/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.6963 - val_accuracy: 0.7914 Epoch 49/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.7048 - val_accuracy: 0.7914 Epoch 50/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7089 - val_accuracy: 0.7861 Epoch 51/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7178 - val_accuracy: 0.7861 Epoch 52/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.7227 - val_accuracy: 0.7861 Epoch 53/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.7294 - val_accuracy: 0.7861 Epoch 54/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7422 - val_accuracy: 0.7861 Epoch 55/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.7392 - val_accuracy: 0.7861 Epoch 56/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.7433 - val_accuracy: 0.7861 Epoch 57/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7461 - val_accuracy: 0.7861 Epoch 58/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7419 - val_accuracy: 0.7861 Epoch 59/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7671 - val_accuracy: 0.7807 Epoch 60/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7671 - val_accuracy: 0.7807 Epoch 61/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7679 - val_accuracy: 0.7807 Epoch 62/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.7797 - val_accuracy: 0.7807 Epoch 63/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7966 - val_accuracy: 0.7807 Epoch 64/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8105 - val_accuracy: 0.7807 Epoch 65/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.8128 - val_accuracy: 0.7807 Epoch 66/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8040 - val_accuracy: 0.7807 Epoch 67/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8085 - val_accuracy: 0.7807 Epoch 68/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8165 - val_accuracy: 0.7807 Epoch 69/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8230 - val_accuracy: 0.7807 Epoch 70/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8232 - val_accuracy: 0.7807 Epoch 71/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8224 - val_accuracy: 0.7807 Epoch 72/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8332 - val_accuracy: 0.7807 Epoch 73/100 57/57 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8399 - val_accuracy: 0.7807 Epoch 74/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8415 - val_accuracy: 0.7807 Epoch 75/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8530 - val_accuracy: 0.7807 Epoch 76/100 57/57 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8581 - val_accuracy: 0.7807 Epoch 77/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8545 - val_accuracy: 0.7807 Epoch 78/100 57/57 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8605 - val_accuracy: 0.7807 Epoch 79/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8688 - val_accuracy: 0.7807 Epoch 80/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8766 - val_accuracy: 0.7807 Epoch 81/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8799 - val_accuracy: 0.7807 Epoch 82/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8800 - val_accuracy: 0.7807 Epoch 83/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8863 - val_accuracy: 0.7807 Epoch 84/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.8866 - val_accuracy: 0.7807 Epoch 85/100 57/57 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.8925 - val_accuracy: 0.7807 Epoch 86/100 57/57 [==============================] - 0s 2ms/step - loss: 9.6562e-04 - accuracy: 1.0000 - val_loss: 0.9019 - val_accuracy: 0.7807 Epoch 87/100 57/57 [==============================] - 0s 1ms/step - loss: 9.2877e-04 - accuracy: 1.0000 - val_loss: 0.9003 - val_accuracy: 0.7807 Epoch 88/100 57/57 [==============================] - 0s 2ms/step - loss: 8.9533e-04 - accuracy: 1.0000 - val_loss: 0.9103 - val_accuracy: 0.7807 Epoch 89/100 57/57 [==============================] - 0s 1ms/step - loss: 8.6348e-04 - accuracy: 1.0000 - val_loss: 0.9125 - val_accuracy: 0.7807 Epoch 90/100 57/57 [==============================] - 0s 2ms/step - loss: 8.3003e-04 - accuracy: 1.0000 - val_loss: 0.9177 - val_accuracy: 0.7807 Epoch 91/100 57/57 [==============================] - 0s 2ms/step - loss: 7.9647e-04 - accuracy: 1.0000 - val_loss: 0.9177 - val_accuracy: 0.7807 Epoch 92/100 57/57 [==============================] - 0s 2ms/step - loss: 7.7104e-04 - accuracy: 1.0000 - val_loss: 0.9197 - val_accuracy: 0.7861 Epoch 93/100 57/57 [==============================] - 0s 2ms/step - loss: 7.4279e-04 - accuracy: 1.0000 - val_loss: 0.9226 - val_accuracy: 0.7861 Epoch 94/100 57/57 [==============================] - 0s 2ms/step - loss: 7.1501e-04 - accuracy: 1.0000 - val_loss: 0.9314 - val_accuracy: 0.7861 Epoch 95/100 57/57 [==============================] - 0s 1ms/step - loss: 6.9016e-04 - accuracy: 1.0000 - val_loss: 0.9365 - val_accuracy: 0.7807 Epoch 96/100 57/57 [==============================] - 0s 2ms/step - loss: 6.6338e-04 - accuracy: 1.0000 - val_loss: 0.9384 - val_accuracy: 0.7807 Epoch 97/100 57/57 [==============================] - 0s 2ms/step - loss: 6.4135e-04 - accuracy: 1.0000 - val_loss: 0.9403 - val_accuracy: 0.7861 Epoch 98/100 57/57 [==============================] - 0s 2ms/step - loss: 6.1557e-04 - accuracy: 1.0000 - val_loss: 0.9418 - val_accuracy: 0.7861 Epoch 99/100 57/57 [==============================] - 0s 2ms/step - loss: 5.9639e-04 - accuracy: 1.0000 - val_loss: 0.9459 - val_accuracy: 0.7861 Epoch 100/100 57/57 [==============================] - 0s 1ms/step - loss: 5.7576e-04 - accuracy: 1.0000 - val_loss: 0.9494 - val_accuracy: 0.7861 . loss, accuracy = model.evaluate(X_train, y_train, verbose=False) print(&quot;Training Accuracy: {:.4f}&quot;.format(accuracy)) loss, accuracy = model.evaluate(X_test, y_test, verbose=False) print(&quot;Testing Accuracy: {:.4f}&quot;.format(accuracy)) . Training Accuracy: 1.0000 Testing Accuracy: 0.7861 . history.history.keys() . dict_keys([&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;]) . def plot_history(history): acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] x = range(1, len(acc) + 1) plt.figure(figsize=(12, 5)) plt.subplot(1, 2, 1) plt.plot(x, acc, &#39;b&#39;, label=&#39;Training acc&#39;) plt.plot(x, val_acc, &#39;r&#39;, label=&#39;Validation acc&#39;) plt.title(&#39;Training and validation accuracy&#39;) plt.legend() plt.subplot(1, 2, 2) plt.plot(x, loss, &#39;b&#39;, label=&#39;Training loss&#39;) plt.plot(x, val_loss, &#39;r&#39;, label=&#39;Validation loss&#39;) plt.title(&#39;Training and validation loss&#39;) plt.legend() . plot_history(history) . What Is a Word Embedding? . Text is considered a form of sequence data similar to time series data that you would have in weather data or financial data. Now you will see how to represent each word as vectors. There are various ways to vectorize text, such as: . Words represented by each word as a vector | Characters represented by each character as a vector | N-grams of words/characters represented as a vector (N-grams are overlapping groups of multiple succeeding words/characters in the text) | . Here, you’ll see how to deal with representing words as vectors which is the common way to use text in neural networks. Two possible ways to represent a word as a vector are one-hot encoding and word embeddings. . One-Hot Encoding . The first way to represent a word as a vector is by creating a so-called one-hot encoding, which is simply done by taking a vector of the length of the vocabulary with an entry for each word in the corpus. . In this way, you have for each word, given it has a spot in the vocabulary, a vector with zeros everywhere except for the corresponding spot for the word which is set to one. . cities = [&#39;London&#39;, &#39;Berlin&#39;, &#39;Berlin&#39;, &#39;New York&#39;, &#39;London&#39;] cities . [&#39;London&#39;, &#39;Berlin&#39;, &#39;Berlin&#39;, &#39;New York&#39;, &#39;London&#39;] . LabelEncoder to encode the list of cities into categorical integer values . encoder = LabelEncoder() city_labels = encoder.fit_transform(cities) city_labels . array([1, 0, 0, 2, 1]) . OneHotEncoder expects each categorical value to be in a separate row, so you’ll need to reshape the array, then you can apply the encoder: . encoder = OneHotEncoder(sparse=False) city_labels = city_labels.reshape((5, 1)) encoder.fit_transform(city_labels) . array([[0., 1., 0.], [1., 0., 0.], [1., 0., 0.], [0., 0., 1.], [0., 1., 0.]]) . Word Embeddings . This method represents words as dense word vectors (also called word embeddings) which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions. . Note that the word embeddings do not understand the text as a human would, but they rather map the statistical structure of the language used in the corpus. Their aim is to map semantic meaning into a geometric space. This geometric space is then called the embedding space. . Now you need to tokenize the data into a format that can be used by the word embeddings. Keras offers a couple of convenience methods for text preprocessing and sequence preprocessing which you can employ to prepare your text. . You can start by using the Tokenizer utility class which can vectorize a text corpus into a list of integers. Each integer maps to a value in a dictionary that encodes the entire corpus, with the keys in the dictionary being the vocabulary terms themselves. You can add the parameter num_words, which is responsible for setting the size of the vocabulary. The most common num_words words will be then kept. . tokenizer = Tokenizer(num_words=5000) tokenizer.fit_on_texts(sentences_train) X_train = tokenizer.texts_to_sequences(sentences_train) X_test = tokenizer.texts_to_sequences(sentences_test) vocab_size = len(tokenizer.word_index) + 1 # Adding 1 because of reserved 0 index print(sentences_train[2]) print(X_train[2]) . I am a fan of his ... This movie sucked really bad. [7, 150, 2, 932, 4, 49, 6, 11, 563, 45, 30] . for word in [&#39;the&#39;, &#39;all&#39;,&#39;fan&#39;]: print(&#39;{}: {}&#39;.format(word, tokenizer.word_index[word])) . the: 1 all: 27 fan: 932 . pad sequences with Keras . maxlen = 100 X_train = pad_sequences(X_train, padding=&#39;post&#39;, maxlen=maxlen) X_test = pad_sequences(X_test, padding=&#39;post&#39;, maxlen=maxlen) print(X_train[0, :]) . [170 116 390 35 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] . Keras Embedding Layer . Now you can use the Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding. You will need the following parameters: . input_dim: the size of the vocabulary | output_dim: the size of the dense vector | input_length: the length of the sequence | . With the Embedding layer we have now a couple of options. One way would be to take the output of the embedding layer and plug it into a Dense layer. In order to do this you have to add a Flatten layer in between that prepares the sequential input for the Dense layer: . embedding_dim = 50 model = Sequential() model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen)) model.add(layers.Flatten()) model.add(layers.Dense(10, activation=&#39;relu&#39;)) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 50) 128750 flatten (Flatten) (None, 5000) 0 dense_2 (Dense) (None, 10) 50010 dense_3 (Dense) (None, 1) 11 ================================================================= Total params: 178,771 Trainable params: 178,771 Non-trainable params: 0 _________________________________________________________________ . history = model.fit(X_train, y_train, epochs=20, verbose=True, validation_data=(X_test, y_test), batch_size=10) loss, accuracy = model.evaluate(X_train, y_train, verbose=False) print(&quot;Training Accuracy: {:.4f}&quot;.format(accuracy)) loss, accuracy = model.evaluate(X_test, y_test, verbose=False) print(&quot;Testing Accuracy: {:.4f}&quot;.format(accuracy)) plot_history(history) . Epoch 1/20 57/57 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.7035 - val_accuracy: 0.4920 Epoch 2/20 57/57 [==============================] - 0s 5ms/step - loss: 0.6594 - accuracy: 0.6132 - val_loss: 0.6850 - val_accuracy: 0.5401 Epoch 3/20 57/57 [==============================] - 0s 3ms/step - loss: 0.5408 - accuracy: 0.8948 - val_loss: 0.6708 - val_accuracy: 0.5829 Epoch 4/20 57/57 [==============================] - 0s 4ms/step - loss: 0.3138 - accuracy: 0.9626 - val_loss: 0.6345 - val_accuracy: 0.6257 Epoch 5/20 57/57 [==============================] - 0s 3ms/step - loss: 0.1500 - accuracy: 0.9857 - val_loss: 0.6239 - val_accuracy: 0.6417 Epoch 6/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9947 - val_loss: 0.6313 - val_accuracy: 0.6364 Epoch 7/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0420 - accuracy: 0.9982 - val_loss: 0.6526 - val_accuracy: 0.6684 Epoch 8/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.7150 - val_accuracy: 0.6417 Epoch 9/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.6624 - val_accuracy: 0.6524 Epoch 10/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.6743 - val_accuracy: 0.6471 Epoch 11/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.6854 - val_accuracy: 0.6417 Epoch 12/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.6970 - val_accuracy: 0.6524 Epoch 13/20 57/57 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.7079 - val_accuracy: 0.6578 Epoch 14/20 57/57 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.7192 - val_accuracy: 0.6578 Epoch 15/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7323 - val_accuracy: 0.6524 Epoch 16/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7446 - val_accuracy: 0.6524 Epoch 17/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.7488 - val_accuracy: 0.6524 Epoch 18/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7617 - val_accuracy: 0.6471 Epoch 19/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7644 - val_accuracy: 0.6578 Epoch 20/20 57/57 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7716 - val_accuracy: 0.6524 Training Accuracy: 1.0000 Testing Accuracy: 0.6524 . Global max/average pooling takes the maximum/average of all features whereas in the other case you have to define the pool size. Keras has again its own layer that you can add in the sequential model: . embedding_dim = 50 model = Sequential() model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen)) model.add(layers.GlobalMaxPool1D()) model.add(layers.Dense(10, activation=&#39;relu&#39;)) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 100, 50) 128750 global_max_pooling1d (Globa (None, 50) 0 lMaxPooling1D) dense_4 (Dense) (None, 10) 510 dense_5 (Dense) (None, 1) 11 ================================================================= Total params: 129,271 Trainable params: 129,271 Non-trainable params: 0 _________________________________________________________________ . history = model.fit(X_train, y_train, epochs=50, verbose=False, validation_data=(X_test, y_test), batch_size=10) loss, accuracy = model.evaluate(X_train, y_train, verbose=False) print(&quot;Training Accuracy: {:.4f}&quot;.format(accuracy)) loss, accuracy = model.evaluate(X_test, y_test, verbose=False) print(&quot;Testing Accuracy: {:.4f}&quot;.format(accuracy)) plot_history(history) . Training Accuracy: 1.0000 Testing Accuracy: 0.7594 . Convolutional Neural Networks (CNN) . Convolutional neural networks or also called convnets are one of the most exciting developments in machine learning in recent years. . They have revolutionized image classification and computer vision by being able to extract features from images and using them in neural networks. The properties that made them useful in image processing makes them also handy for sequence processing. You can imagine a CNN as a specialized neural network that is able to detect specific patterns. . A CNN has hidden layers which are called convolutional layers. When you think of images, a computer has to deal with a two dimensional matrix of numbers and therefore you need some way to detect features in this matrix. These convolutional layers are able to detect edges, corners and other kinds of textures which makes them such a special tool. The convolutional layer consists of multiple filters which are slid across the image and are able to detect specific features. . embedding_dim = 100 model = Sequential() model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen)) model.add(layers.Conv1D(128, 5, activation=&#39;relu&#39;)) model.add(layers.GlobalMaxPooling1D()) model.add(layers.Dense(10, activation=&#39;relu&#39;)) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_2 (Embedding) (None, 100, 100) 257500 conv1d (Conv1D) (None, 96, 128) 64128 global_max_pooling1d_1 (Glo (None, 128) 0 balMaxPooling1D) dense_6 (Dense) (None, 10) 1290 dense_7 (Dense) (None, 1) 11 ================================================================= Total params: 322,929 Trainable params: 322,929 Non-trainable params: 0 _________________________________________________________________ . history = model.fit(X_train, y_train, epochs=10, verbose=False, validation_data=(X_test, y_test), batch_size=10) loss, accuracy = model.evaluate(X_train, y_train, verbose=False) print(&quot;Training Accuracy: {:.4f}&quot;.format(accuracy)) loss, accuracy = model.evaluate(X_test, y_test, verbose=False) print(&quot;Testing Accuracy: {:.4f}&quot;.format(accuracy)) plot_history(history) . Training Accuracy: 1.0000 Testing Accuracy: 0.7807 . Hyperparameters Optimization . One crucial steps of deep learning and working with neural networks is hyperparameter optimization. . As you saw in the models that we have used so far, even with simpler ones, you had a large number of parameters to tweak and choose from. Those parameters are called hyperparameters. This is the most time consuming part of machine learning and sadly there are no one-fits-all solutions ready. . One popular method for hyperparameter optimization is grid search. What this method does is it takes lists of parameters and it runs the model with each parameter combination that it can find. It is the most thorough way but also the most computationally heavy way to do this. Another common way,random search, which you’ll see in action here, simply takes random combinations of parameters. . In order to apply random search with Keras, you will need to use the KerasClassifier which serves as a wrapper for the scikit-learn API. With this wrapper you are able to use the various tools available with scikit-learn like cross-validation. The class that you need is RandomizedSearchCV which implements random search with cross-validation. Cross-validation is a way to validate the model and take the whole data set and separate it into multiple testing and training data sets. . There are various types of cross-validation. One type is the k-fold cross-validation. In this type the data set is partitioned into k equal sized sets where one set is used for testing and the rest of the partitions are used for training. This enables you to run k different runs, where each partition is once used as a testing set. So, the higher k is the more accurate the model evaluation is, but the smaller each testing set is. . def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen): model = Sequential() model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen)) model.add(layers.Conv1D(num_filters, kernel_size, activation=&#39;relu&#39;)) model.add(layers.GlobalMaxPooling1D()) model.add(layers.Dense(10, activation=&#39;relu&#39;)) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return model . param_grid = dict(num_filters=[32, 64, 128], kernel_size=[3, 5, 7], vocab_size=[5000], embedding_dim=[50], maxlen=[100]) . epochs = 20 embedding_dim = 50 maxlen = 100 output_file = &#39;output.txt&#39; # Run grid search for each source (yelp, amazon, imdb) for source, frame in df.groupby(&#39;source&#39;): print(&#39;Running grid search for data set :&#39;, source) sentences = df[&#39;sentence&#39;].values y = df[&#39;label&#39;].values # Train-test split sentences_train, sentences_test, y_train, y_test = train_test_split( sentences, y, test_size=0.25, random_state=1000) # Tokenize words tokenizer = Tokenizer(num_words=5000) tokenizer.fit_on_texts(sentences_train) X_train = tokenizer.texts_to_sequences(sentences_train) X_test = tokenizer.texts_to_sequences(sentences_test) # Adding 1 because of reserved 0 index vocab_size = len(tokenizer.word_index) + 1 # Pad sequences with zeros X_train = pad_sequences(X_train, padding=&#39;post&#39;, maxlen=maxlen) X_test = pad_sequences(X_test, padding=&#39;post&#39;, maxlen=maxlen) # Parameter grid for grid search param_grid = dict(num_filters=[32, 64, 128], kernel_size=[3, 5, 7], vocab_size=[vocab_size], embedding_dim=[embedding_dim], maxlen=[maxlen]) model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=10, verbose=False) grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=4, verbose=1, n_iter=5) grid_result = grid.fit(X_train, y_train) # Evaluate testing set test_accuracy = grid.score(X_test, y_test) # Save and evaluate results # prompt = input(f&#39;finished {source}; write to file and proceed? [y/n]&#39;) # if prompt.lower() not in {&#39;y&#39;, &#39;true&#39;, &#39;yes&#39;}: # break # with open(output_file, &#39;w+&#39;) as f: s = (&#39;Running {} data set nBest Accuracy : &#39; &#39;{:.4f} n{} nTest Accuracy : {:.4f} n n&#39;) output_string = s.format( source, grid_result.best_score_, grid_result.best_params_, test_accuracy) print(output_string) # f.write(output_string) . Running grid search for data set : amazon Fitting 4 folds for each of 5 candidates, totalling 20 fits . /var/folders/lm/tqm129c507j4njpg3k8cmqf40000gp/T/ipykernel_98623/297795620.py:36: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating. model = KerasClassifier(build_fn=create_model, [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 20 out of 20 | elapsed: 3.2min finished . Running amazon data set Best Accuracy : 0.8113 {&#39;vocab_size&#39;: 4603, &#39;num_filters&#39;: 32, &#39;maxlen&#39;: 100, &#39;kernel_size&#39;: 3, &#39;embedding_dim&#39;: 50} Test Accuracy : 0.8399 Running grid search for data set : imdb Fitting 4 folds for each of 5 candidates, totalling 20 fits . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 20 out of 20 | elapsed: 3.2min finished . Running imdb data set Best Accuracy : 0.8142 {&#39;vocab_size&#39;: 4603, &#39;num_filters&#39;: 32, &#39;maxlen&#39;: 100, &#39;kernel_size&#39;: 5, &#39;embedding_dim&#39;: 50} Test Accuracy : 0.8341 Running grid search for data set : yelp Fitting 4 folds for each of 5 candidates, totalling 20 fits . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 20 out of 20 | elapsed: 3.6min finished . Running yelp data set Best Accuracy : 0.8142 {&#39;vocab_size&#39;: 4603, &#39;num_filters&#39;: 32, &#39;maxlen&#39;: 100, &#39;kernel_size&#39;: 7, &#39;embedding_dim&#39;: 50} Test Accuracy : 0.8297 .",
            "url": "https://taocao.github.io/ml/jupyter/2022/09/04/text-classification-with-python-and-keras.html",
            "relUrl": "/jupyter/2022/09/04/text-classification-with-python-and-keras.html",
            "date": " • Sep 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Basic regression - Predict fuel efficiency",
            "content": "In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture). . This tutorial uses the classic Auto MPG dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight. . This example uses the Keras API. (Visit the Keras tutorials and guides to learn more.) . import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns # Make NumPy printouts easier to read. np.set_printoptions(precision=3, suppress=True) . import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers print(tf.__version__) . 2.6.0 . The Auto MPG dataset . The dataset is available from the UCI Machine Learning Repository. . Get the data . First download and import the dataset using pandas: . url = &#39;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&#39; column_names = [&#39;MPG&#39;, &#39;Cylinders&#39;, &#39;Displacement&#39;, &#39;Horsepower&#39;, &#39;Weight&#39;, &#39;Acceleration&#39;, &#39;Model Year&#39;, &#39;Origin&#39;] raw_dataset = pd.read_csv(url, names=column_names, na_values=&#39;?&#39;, comment=&#39; t&#39;, sep=&#39; &#39;, skipinitialspace=True) . dataset = raw_dataset.copy() pd.DataFrame(dataset.tail()) . MPG Cylinders Displacement Horsepower Weight Acceleration Model Year Origin . 393 27.0 | 4 | 140.0 | 86.0 | 2790.0 | 15.6 | 82 | 1 | . 394 44.0 | 4 | 97.0 | 52.0 | 2130.0 | 24.6 | 82 | 2 | . 395 32.0 | 4 | 135.0 | 84.0 | 2295.0 | 11.6 | 82 | 1 | . 396 28.0 | 4 | 120.0 | 79.0 | 2625.0 | 18.6 | 82 | 1 | . 397 31.0 | 4 | 119.0 | 82.0 | 2720.0 | 19.4 | 82 | 1 | . Clean the data . The dataset contains a few unknown values: . dataset.isna().sum() . MPG 0 Cylinders 0 Displacement 0 Horsepower 6 Weight 0 Acceleration 0 Model Year 0 Origin 0 dtype: int64 . Drop those rows to keep this initial tutorial simple: . dataset = dataset.dropna() . The &quot;Origin&quot; column is categorical, not numeric. So the next step is to one-hot encode the values in the column with pd.get_dummies. . Note: You can set up the tf.keras.Model to do this kind of transformation for you but that&#39;s beyond the scope of this tutorial. Check out the Classify structured data using Keras preprocessing layers or Load CSV data tutorials for examples. . dataset[&#39;Origin&#39;] = dataset[&#39;Origin&#39;].map({1: &#39;USA&#39;, 2: &#39;Europe&#39;, 3: &#39;Japan&#39;}) . dataset = pd.get_dummies(dataset, columns=[&#39;Origin&#39;], prefix=&#39;&#39;, prefix_sep=&#39;&#39;) pd.DataFrame(dataset.tail()) . MPG Cylinders Displacement Horsepower Weight Acceleration Model Year 1 2 3 . 393 27.0 | 4 | 140.0 | 86.0 | 2790.0 | 15.6 | 82 | 1 | 0 | 0 | . 394 44.0 | 4 | 97.0 | 52.0 | 2130.0 | 24.6 | 82 | 0 | 1 | 0 | . 395 32.0 | 4 | 135.0 | 84.0 | 2295.0 | 11.6 | 82 | 1 | 0 | 0 | . 396 28.0 | 4 | 120.0 | 79.0 | 2625.0 | 18.6 | 82 | 1 | 0 | 0 | . 397 31.0 | 4 | 119.0 | 82.0 | 2720.0 | 19.4 | 82 | 1 | 0 | 0 | . Split the data into training and test sets . Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models. . train_dataset = dataset.sample(frac=0.8, random_state=0) test_dataset = dataset.drop(train_dataset.index) . Inspect the data . Review the joint distribution of a few pairs of columns from the training set. . The top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other. . sns.pairplot(train_dataset[[&#39;MPG&#39;, &#39;Cylinders&#39;, &#39;Displacement&#39;, &#39;Weight&#39;]], diag_kind=&#39;kde&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x7ff3f5cc0a00&gt; . Let&#39;s also check the overall statistics. Note how each feature covers a very different range: . train_dataset.describe().transpose() . count mean std min 25% 50% 75% max . MPG 314.0 | 23.310510 | 7.728652 | 10.0 | 17.00 | 22.0 | 28.95 | 46.6 | . Cylinders 314.0 | 5.477707 | 1.699788 | 3.0 | 4.00 | 4.0 | 8.00 | 8.0 | . Displacement 314.0 | 195.318471 | 104.331589 | 68.0 | 105.50 | 151.0 | 265.75 | 455.0 | . Horsepower 314.0 | 104.869427 | 38.096214 | 46.0 | 76.25 | 94.5 | 128.00 | 225.0 | . Weight 314.0 | 2990.251592 | 843.898596 | 1649.0 | 2256.50 | 2822.5 | 3608.00 | 5140.0 | . Acceleration 314.0 | 15.559236 | 2.789230 | 8.0 | 13.80 | 15.5 | 17.20 | 24.8 | . Model Year 314.0 | 75.898089 | 3.675642 | 70.0 | 73.00 | 76.0 | 79.00 | 82.0 | . Europe 314.0 | 0.178344 | 0.383413 | 0.0 | 0.00 | 0.0 | 0.00 | 1.0 | . Japan 314.0 | 0.197452 | 0.398712 | 0.0 | 0.00 | 0.0 | 0.00 | 1.0 | . USA 314.0 | 0.624204 | 0.485101 | 0.0 | 0.00 | 1.0 | 1.00 | 1.0 | . Split features from labels . Separate the target value—the &quot;label&quot;—from the features. This label is the value that you will train the model to predict. . train_features = train_dataset.copy() test_features = test_dataset.copy() train_labels = train_features.pop(&#39;MPG&#39;) test_labels = test_features.pop(&#39;MPG&#39;) . Normalization . In the table of statistics it&#39;s easy to see how different the ranges of each feature are: . train_dataset.describe().transpose()[[&#39;mean&#39;, &#39;std&#39;]] . mean std . MPG 23.310510 | 7.728652 | . Cylinders 5.477707 | 1.699788 | . Displacement 195.318471 | 104.331589 | . Horsepower 104.869427 | 38.096214 | . Weight 2990.251592 | 843.898596 | . Acceleration 15.559236 | 2.789230 | . Model Year 75.898089 | 3.675642 | . Europe 0.178344 | 0.383413 | . Japan 0.197452 | 0.398712 | . USA 0.624204 | 0.485101 | . It is good practice to normalize features that use different scales and ranges. . One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs. . Although a model might converge without feature normalization, normalization makes training much more stable. . Note: There is no advantage to normalizing the one-hot features—it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the Working with preprocessing layers guide and the Classify structured data using Keras preprocessing layers tutorial. . The Normalization layer . The tf.keras.layers.Normalization is a clean and simple way to add feature normalization into your model. . The first step is to create the layer: . normalizer = tf.keras.layers.Normalization(axis=-1) . Then, fit the state of the preprocessing layer to the data by calling Normalization.adapt: . normalizer.adapt(np.array(train_features)) . 2022-08-25 17:26:41.473604: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-08-25 17:26:41.510157: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2) . Calculate the mean and variance, and store them in the layer: . print(normalizer.mean.numpy()) . [[ 5.478 195.318 104.869 2990.252 15.559 75.898 0.178 0.197 0.624]] . When the layer is called, it returns the input data, with each feature independently normalized: . first = np.array(train_features[:1]) with np.printoptions(precision=2, suppress=True): print(&#39;First example:&#39;, first) print() print(&#39;Normalized:&#39;, normalizer(first).numpy()) . First example: [[ 4. 90. 75. 2125. 14.5 74. 0. 0. 1. ]] Normalized: [[-0.87 -1.01 -0.79 -1.03 -0.38 -0.52 -0.47 -0.5 0.78]] . Linear regression . Before building a deep neural network model, start with linear regression using one and several variables. . Linear regression with one variable . Begin with a single-variable linear regression to predict &#39;MPG&#39; from &#39;Horsepower&#39;. . Training a model with tf.keras typically starts by defining the model architecture. Use a tf.keras.Sequential model, which represents a sequence of steps. . There are two steps in your single-variable linear regression model: . Normalize the &#39;Horsepower&#39; input features using the tf.keras.layers.Normalization preprocessing layer. | Apply a linear transformation ($y = mx+b$) to produce 1 output using a linear layer (tf.keras.layers.Dense). | . The number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time. . First, create a NumPy array made of the &#39;Horsepower&#39; features. Then, instantiate the tf.keras.layers.Normalization and fit its state to the horsepower data: . horsepower = np.array(train_features[&#39;Horsepower&#39;]) horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None) horsepower_normalizer.adapt(horsepower) . Build the Keras Sequential model: . horsepower_model = tf.keras.Sequential([ horsepower_normalizer, layers.Dense(units=1) ]) horsepower_model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= normalization_1 (Normalizati (None, 1) 3 _________________________________________________________________ dense (Dense) (None, 1) 2 ================================================================= Total params: 5 Trainable params: 2 Non-trainable params: 3 _________________________________________________________________ . This model will predict &#39;MPG&#39; from &#39;Horsepower&#39;. . Run the untrained model on the first 10 &#39;Horsepower&#39; values. The output won&#39;t be good, but notice that it has the expected shape of (10, 1): . horsepower_model.predict(horsepower[:10]) . array([[ 0.811], [ 0.458], [-1.496], [ 1.136], [ 1.028], [ 0.404], [ 1.218], [ 1.028], [ 0.268], [ 0.458]], dtype=float32) . Once the model is built, configure the training procedure using the Keras Model.compile method. The most important arguments to compile are the loss and the optimizer, since these define what will be optimized (mean_absolute_error) and how (using the tf.keras.optimizers.Adam). . horsepower_model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=&#39;mean_absolute_error&#39;) . Use Keras Model.fit to execute the training for 100 epochs: . %%time history = horsepower_model.fit( train_features[&#39;Horsepower&#39;], train_labels, epochs=100, # Suppress logging. verbose=0, # Calculate validation results on 20% of the training data. validation_split = 0.2) . CPU times: user 2.12 s, sys: 228 ms, total: 2.34 s Wall time: 2.13 s . Visualize the model&#39;s training progress using the stats stored in the history object: . hist = pd.DataFrame(history.history) hist[&#39;epoch&#39;] = history.epoch hist.tail() . loss val_loss epoch . 95 3.806936 | 4.210271 | 95 | . 96 3.800425 | 4.172709 | 96 | . 97 3.808563 | 4.146346 | 97 | . 98 3.806711 | 4.158387 | 98 | . 99 3.803950 | 4.172227 | 99 | . def plot_loss(history): plt.plot(history.history[&#39;loss&#39;], label=&#39;loss&#39;) plt.plot(history.history[&#39;val_loss&#39;], label=&#39;val_loss&#39;) plt.ylim([0, 10]) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Error [MPG]&#39;) plt.legend() plt.grid(True) . plot_loss(history) . Collect the results on the test set for later: . test_results = {} test_results[&#39;horsepower_model&#39;] = horsepower_model.evaluate( test_features[&#39;Horsepower&#39;], test_labels, verbose=0) . Since this is a single variable regression, it&#39;s easy to view the model&#39;s predictions as a function of the input: . x = tf.linspace(0.0, 250, 251) y = horsepower_model.predict(x) . def plot_horsepower(x, y): plt.scatter(train_features[&#39;Horsepower&#39;], train_labels, label=&#39;Data&#39;) plt.plot(x, y, color=&#39;k&#39;, label=&#39;Predictions&#39;) plt.xlabel(&#39;Horsepower&#39;) plt.ylabel(&#39;MPG&#39;) plt.legend() . plot_horsepower(x, y) . Linear regression with multiple inputs . You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector. . Create a two-step Keras Sequential model again with the first layer being normalizer (tf.keras.layers.Normalization(axis=-1)) you defined earlier and adapted to the whole dataset: . linear_model = tf.keras.Sequential([ normalizer, layers.Dense(units=1) ]) . When you call Model.predict on a batch of inputs, it produces units=1 outputs for each example: . linear_model.predict(train_features[:10]) . array([[ 0.419], [ 0.996], [-0.423], [ 1.14 ], [ 1.444], [ 0.47 ], [ 1.364], [-0.828], [ 0.059], [ 0.718]], dtype=float32) . When you call the model, its weight matrices will be built—check that the kernel weights (the $m$ in $y=mx+b$) have a shape of (9, 1): . linear_model.layers[1].kernel . &lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(9, 1) dtype=float32, numpy= array([[-0.625], [ 0.111], [-0.462], [ 0.533], [-0.157], [ 0.262], [-0.635], [ 0.458], [ 0.229]], dtype=float32)&gt; . Configure the model with Keras Model.compile and train with Model.fit for 100 epochs: . linear_model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=&#39;mean_absolute_error&#39;) . %%time history = linear_model.fit( train_features, train_labels, epochs=100, # Suppress logging. verbose=0, # Calculate validation results on 20% of the training data. validation_split = 0.2) . CPU times: user 2.1 s, sys: 223 ms, total: 2.33 s Wall time: 2.2 s . Using all the inputs in this regression model achieves a much lower training and validation error than the horsepower_model, which had one input: . plot_loss(history) . Collect the results on the test set for later: . test_results[&#39;linear_model&#39;] = linear_model.evaluate( test_features, test_labels, verbose=0) . Regression with a deep neural network (DNN) . In the previous section, you implemented two linear models for single and multiple inputs. . Here, you will implement single-input and multiple-input DNN models. . The code is basically the same except the model is expanded to include some &quot;hidden&quot; non-linear layers. The name &quot;hidden&quot; here just means not directly connected to the inputs or outputs. . These models will contain a few more layers than the linear model: . The normalization layer, as before (with horsepower_normalizer for a single-input model and normalizer for a multiple-input model). | Two hidden, non-linear, Dense layers with the ReLU (relu) activation function nonlinearity. | A linear Dense single-output layer. | . Both models will use the same training procedure, so the compile method is included in the build_and_compile_model function below. . def build_and_compile_model(norm): model = keras.Sequential([ norm, layers.Dense(64, activation=&#39;relu&#39;), layers.Dense(64, activation=&#39;relu&#39;), layers.Dense(1) ]) model.compile(loss=&#39;mean_absolute_error&#39;, optimizer=tf.keras.optimizers.Adam(0.001)) return model . Regression using a DNN and a single input . Create a DNN model with only &#39;Horsepower&#39; as input and horsepower_normalizer (defined earlier) as the normalization layer: . dnn_horsepower_model = build_and_compile_model(horsepower_normalizer) . This model has quite a few more trainable parameters than the linear models: . dnn_horsepower_model.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= normalization_1 (Normalizati (None, 1) 3 _________________________________________________________________ dense_2 (Dense) (None, 64) 128 _________________________________________________________________ dense_3 (Dense) (None, 64) 4160 _________________________________________________________________ dense_4 (Dense) (None, 1) 65 ================================================================= Total params: 4,356 Trainable params: 4,353 Non-trainable params: 3 _________________________________________________________________ . Train the model with Keras Model.fit: . %%time history = dnn_horsepower_model.fit( train_features[&#39;Horsepower&#39;], train_labels, validation_split=0.2, verbose=0, epochs=100) . CPU times: user 2.27 s, sys: 360 ms, total: 2.63 s Wall time: 2.07 s . This model does slightly better than the linear single-input horsepower_model: . plot_loss(history) . If you plot the predictions as a function of &#39;Horsepower&#39;, you should notice how this model takes advantage of the nonlinearity provided by the hidden layers: . x = tf.linspace(0.0, 250, 251) y = dnn_horsepower_model.predict(x) . plot_horsepower(x, y) . Collect the results on the test set for later: . test_results[&#39;dnn_horsepower_model&#39;] = dnn_horsepower_model.evaluate( test_features[&#39;Horsepower&#39;], test_labels, verbose=0) . Regression using a DNN and multiple inputs . Repeat the previous process using all the inputs. The model&#39;s performance slightly improves on the validation dataset. . dnn_model = build_and_compile_model(normalizer) dnn_model.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= normalization (Normalization (None, 9) 19 _________________________________________________________________ dense_5 (Dense) (None, 64) 640 _________________________________________________________________ dense_6 (Dense) (None, 64) 4160 _________________________________________________________________ dense_7 (Dense) (None, 1) 65 ================================================================= Total params: 4,884 Trainable params: 4,865 Non-trainable params: 19 _________________________________________________________________ . %%time history = dnn_model.fit( train_features, train_labels, validation_split=0.2, verbose=0, epochs=100) . CPU times: user 2.27 s, sys: 378 ms, total: 2.64 s Wall time: 2.09 s . plot_loss(history) . Collect the results on the test set: . test_results[&#39;dnn_model&#39;] = dnn_model.evaluate(test_features, test_labels, verbose=0) . Performance . Since all models have been trained, you can review their test set performance: . pd.DataFrame(test_results, index=[&#39;Mean absolute error [MPG]&#39;]).T . Mean absolute error [MPG] . horsepower_model 3.654703 | . linear_model 2.484979 | . dnn_horsepower_model 2.935724 | . dnn_model 1.743666 | . These results match the validation error observed during training. . Make predictions . You can now make predictions with the dnn_model on the test set using Keras Model.predict and review the loss: . test_predictions = dnn_model.predict(test_features).flatten() a = plt.axes(aspect=&#39;equal&#39;) plt.scatter(test_labels, test_predictions) plt.xlabel(&#39;True Values [MPG]&#39;) plt.ylabel(&#39;Predictions [MPG]&#39;) lims = [0, 50] plt.xlim(lims) plt.ylim(lims) _ = plt.plot(lims, lims) . It appears that the model predicts reasonably well. . Now, check the error distribution: . error = test_predictions - test_labels plt.hist(error, bins=25) plt.xlabel(&#39;Prediction Error [MPG]&#39;) _ = plt.ylabel(&#39;Count&#39;) . If you&#39;re happy with the model, save it for later use with Model.save: . dnn_model.save(&#39;dnn_model&#39;) . 2022-08-25 17:26:54.351180: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. . INFO:tensorflow:Assets written to: dnn_model/assets . If you reload the model, it gives identical output: . reloaded = tf.keras.models.load_model(&#39;dnn_model&#39;) test_results[&#39;reloaded&#39;] = reloaded.evaluate( test_features, test_labels, verbose=0) . pd.DataFrame(test_results, index=[&#39;Mean absolute error [MPG]&#39;]).T . Mean absolute error [MPG] . horsepower_model 3.654703 | . linear_model 2.484979 | . dnn_horsepower_model 2.935724 | . dnn_model 1.743666 | . reloaded 1.743666 | . Conclusion . This notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help: . Mean squared error (MSE) (tf.keras.losses.MeanSquaredError) and mean absolute error (MAE) (tf.keras.losses.MeanAbsoluteError) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems. | Similarly, evaluation metrics used for regression differ from classification. | When numeric input data features have values with different ranges, each feature should be scaled independently to the same range. | Overfitting is a common problem for DNN models, though it wasn&#39;t a problem for this tutorial. Visit the Overfit and underfit tutorial for more help with this. | .",
            "url": "https://taocao.github.io/ml/jupyter/2022/09/04/regression-example.html",
            "relUrl": "/jupyter/2022/09/04/regression-example.html",
            "date": " • Sep 4, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Best practices when programming a deep learning model",
            "content": "best practices when programming a deep learning model. These practices mostly refer to how we can write organized, modularized, and extensible python code. . Most of them aren’t exclusive for machine learning applications but can be utilized on all sorts of python projects. But here we will see how we can apply them in deep learning using a hands-on programming approach. . Machine learning code is ordinary software and should always be treated as one. Therefore, as ordinary code it has a project structure, a documentation and design principles such as object-oriented programming. . Devops for ML “This key chapter puts the “engineering” in data engineering. DevOps practices allow us to build reliable, reproducible systems. One of the principles you will see repeated throughout the book is tracking everything in source control and deploying everything automatically.” . Excerpt From Data Engineering on Azure Riscutia, Vlad; . ===== https://theaisummer.com/deep-learning-production/ . It’s time. You spent hours reading research papers, experimenting on different data, testing the accuracy of different models but you got it. After training your model locally, you’ve seen some pretty awesome results and you’re convinced that your model is ready to go. What’s next? The next step is to take your experimentation code and migrate it into an actual python project with proper structure, unit tests, static code analysis, parallel processing etc. . In terms of software, the flow is something like this: the user uploads the image to the browser, the browser sends the image to our backend, the UNet predicts the segmented image and we send the image back to the user’s browser, where it is rendered. . ￼ After watching the above system, there are many questions that come in our mind: . What will happen if many users ask the model at the same time? | What will happen if for some reason the model crashes? | If it contains a bug we haven’t previously seen? | If the model is too slow and the user has to wait for too long to get a response? | And how can we update our model? | Can we retrain it on new data? | Can we get some feedback if the model performs well? | Can we see how fast or slow the model is? | How much memory does it use? | Where do we store the model weights? | What about the training data? | What if the user sends a malicious request instead of a proper image? | . I can go on forever but I’m sure you get my point. Most of these questions will be answered in the following articles. That’s why in most real-life cases, the final architecture will be something like this: . ￼ . Ok, we won’t go that far. That’s a whole startup up there. But we will cover many points focusing on optimizing the software at first and touching some of these parts on our latest articles. . Software design principles But to develop our software, we need to have in mind a rough idea of our end goal and what we want to achieve. Every system should be built based on some basic principles: . Separation of concerns: The system should be modularized into different components with each component being a separate maintainable, reusable and extensible entity. | Scalability: The system needs to be able to scale as the traffic increases | Reliability: The system should continue to be functional even if there is software of hardware failure | Availability: The system needs to continue operating at all times | Simplicity: The system has to be as simple and intuitive as possible Given the aforementioned principles, let’s discuss the above image: Each user sends a request to our backend. To be able to handle all the simultaneous requests, first we need a Load Balancer (LB) to handle the traffic and route the requests to the proper server instance. We maintain more than one instance of the model in order to achieve both scalability and availability. Note that these might be physical instances, virtual machines or docker containers and they are organized by a distributed system orchestrator (such as Kubernetes) As you can probably see, each instance may have more than one threads or parallel processes. We need that to make sure that the model is fast enough to return a response in real time. A major problem most machine learning systems face is retraining because in order for the model to stay up to date, you should constantly update the model weights based on the new data. In our case, this pipeline consists of: | A database (DB) to save the requests, the responses and all relative data | A message queue (MQ) to send them in the database in an asynchronous manner ( to keep the system reliable and available) | Data jobs to preprocess and transform the data in the correct format so that can be used from our model | Retrainer instances that execute the actual training based on the saved data | . After retraining the model, the new version will gradually replace all the UNet instances. That way we build deep learning models versioning functionality, where we always update the model with the newest version. (Really? In this case, we need to make sure the newest version is the best, how ?) . Finally, we need some sort of monitoring and logging to have a complete image of what’s happening in our system, to catch errors quickly and to discover bottlenecks easily. . Get ready for Software Engineering . understand the whys of a modern deep learning system. | The second one requires some mental effort because deep learning architectures are rather complex systems. But the main thing that you need to remember are the 5 principles as they will accompany us down the road. We are all set up and ready to begin our journey towards deploying our UNet model and serve it to our users. As I said in the beginning, brace yourself to go deep into programming deep learning apps, to dive into details that you probably never thought of and most of all to enjoy the process. . https://github.com/donnemartin/system-design-primer . https://theaisummer.com/best-practices-deep-learning-code/ . Project Structure One very important aspect when writing code is how you structure your project. A good structure should obey the “Separation of concerns” principle in terms that each functionality should be a distinct component. In this way, it can be easily modified and extended without breaking other parts of the code. Moreover, it can also be reused in many places without the need to write duplicate code. . The way I like to organize most of my deep learning projects is something like this : . ￼ . And that, of course, is my personal preference. Feel free to play around with this until you find what suits you best. . Python modules and packages . Notice that in the project structure that I presented, each folder is a separate module that can be imported in other modules just by doing “import module”. Here, we should make a quick distinction between what python calls module and what package. A module is simply a file containing Python code. A package, however, is like a directory that holds sub-packages and modules. In order for a package to be importable, it should contain a init.py file ( even if it’s empty). That’s not the case for modules. Thus, in our case each folder is a package and it contains an “init” file. . In our example, we have 8 different packages: . configs: in configs we define every single thing that can be configurable and can be changed in the future. Good examples are training hyperparameters, folder paths, the model architecture, metrics, flags. A very simple config is something like this: CFG = { &quot;data&quot;: { &quot;path&quot;: &quot;oxford_iiit_pet:3.*.*&quot;, &quot;image_size&quot;: 128, &quot;load_with_info&quot;: True . }, &quot;train&quot;: { &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 1000, &quot;epoches&quot;: 20, &quot;val_subsplits&quot;: 5, &quot;optimizer&quot;: { &quot;type&quot;: &quot;adam&quot; }, &quot;metrics&quot;: [&quot;accuracy&quot;] . }, &quot;model&quot;: { &quot;input&quot;: [128, 128, 3], &quot;up_stack&quot;: { &quot;layer_1&quot;: 512, &quot;layer_2&quot;: 256, &quot;layer_3&quot;: 128, &quot;layer_4&quot;: 64, &quot;kernels&quot;: 3 }, &quot;output&quot;: 3 . } } | dataloader is quite self-explanatory. All the data loading and data preprocessing classes and functions live here. | evaluation is a collection of code that aims to evaluate the performance and accuracy of our model. | executor: in this folder, we usually have all the functions and scripts that train the model or use it to predict something in different environments. And by different environments I mean: executors for GPUs, executors for distributed systems. This package is our connection with the outer world and it’s what our “main.py” will use. | model contains the actual deep learning code (we talk about tensorflow, pytorch etc) | notebooks include all of our jupyter/colab notebooks in one place. | ops: this one is not always needed, as it includes operations not related with machine learning such as algebraic transformations, image manipulation techniques or maybe graph operations. | utils: utilities functions that are used in more than one places and everything that don’t fall in on the above come here. Now that we have our project well structured and all, we can begin to see how our code should look like on a lower level. |",
            "url": "https://taocao.github.io/ml/jupyter/2022/09/04/best-practices-ML-programming.html",
            "relUrl": "/jupyter/2022/09/04/best-practices-ML-programming.html",
            "date": " • Sep 4, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . df = pd.read_json(movies) # load movies data df.columns = [x.replace(&#39; &#39;, &#39;_&#39;) for x in df.columns.values] genres = df[&#39;Major_Genre&#39;].unique() # get unique field values genres = list(filter(lambda d: d is not None, genres)) # filter out None values genres.sort() # sort alphabetically . . mpaa = [&#39;G&#39;, &#39;PG&#39;, &#39;PG-13&#39;, &#39;R&#39;, &#39;NC-17&#39;, &#39;Not Rated&#39;] . . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](www.gstatic.com/images/branding/googlelogo/2x/googlelogo_color_284x96dp.png) . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . curricula in engineering scienc have little exposure to data methods or optimization. computer scientists and statisticians have little exposure to dynamical systems and control. Our goal is to provide entry point to applied data science for both of these groups of students. https://t.co/ozbDG9ay2l . &mdash; Tao Cao (@taocds) May 28, 2022 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://taocao.github.io/ml/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://taocao.github.io/ml/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://taocao.github.io/ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://taocao.github.io/ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}